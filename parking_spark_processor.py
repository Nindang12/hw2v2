"""
FILE: parking_spark_processor.py
M√î T·∫¢: Camera-based Parking Fee Calculator

LOGIC CAMERA TH·ª∞C T·∫æ:
- Camera ·ªü m·ªói v·ªã tr√≠ (A1, A2, B1, ...) detect xe
- ENTERING: Xe v√†o c·ªïng ‚Üí CH∆ØA t√≠nh ph√≠
- PARKED t·∫°i A1: Camera A1 detect xe ‚Üí B·∫ÆT ƒê·∫¶U t√≠nh ph√≠
- MOVING: Camera kh√¥ng th·∫•y xe n·ªØa ‚Üí V·∫™N t√≠nh ph√≠ (xe di chuy·ªÉn trong b√£i)
- PARKED t·∫°i B5: Camera B5 detect xe ‚Üí TI·∫æP T·ª§C t√≠nh t·ª´ l·∫ßn PARKED ƒë·∫ßu
- EXITING: Xe ra ‚Üí CH·ªêT ph√≠

GroupBy license_plate: Track xe theo bi·ªÉn s·ªë
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import logging
import os
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===========================
# CONFIG
# ===========================
KAFKA_BOOTSTRAP = "192.168.1.9:9092"
KAFKA_TOPIC = "parking-events"
DEFAULT_PRICE_PER_10MIN = 5000
PRICE_CONFIG_FILE = "/opt/shared/parking_price_config.json"
CHECKPOINT_DIR = "/tmp/checkpoint_parking"

def load_price_config():
    """Load gi√° t·ª´ config file"""
    try:
        if os.path.exists(PRICE_CONFIG_FILE):
            with open(PRICE_CONFIG_FILE, 'r') as f:
                config = json.load(f)
                price = config.get("price_per_10min", DEFAULT_PRICE_PER_10MIN)
                logger.info(f"Loaded price from config: {price:,} VND per 10 minutes")
                return price
    except Exception as e:
        logger.warning(f"Error loading price config: {e}, using default: {DEFAULT_PRICE_PER_10MIN}")
    return DEFAULT_PRICE_PER_10MIN

# Load price on startup
PRICE_PER_10MIN = load_price_config()

# ===========================
# SPARK
# ===========================
spark = SparkSession.builder \
    .appName("ParkingCameraSystem") \
    .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_DIR) \
    .config("spark.sql.shuffle.partitions", "2") \
    .config("spark.sql.adaptive.enabled", "false") \
    .master("spark://192.168.1.13:7077") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ===========================
# SCHEMA & READ KAFKA
# ===========================
kafka_schema = StructType([
    StructField("timestamp", StringType(), True),
    StructField("timestamp_unix", LongType(), True),
    StructField("license_plate", StringType(), True),
    StructField("location", StringType(), True),
    StructField("status_code", StringType(), True)
])

kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load()

parsed_df = kafka_df.select(
    from_json(col("value").cast("string"), kafka_schema).alias("data")
).select("data.*")

events_df = parsed_df.withColumn(
    "event_time",
    to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss")
)

# ===========================
# LOGIC: B·ªè ENTERING, ch·ªâ x·ª≠ l√Ω t·ª´ PARKED tr·ªü ƒëi
# ===========================
watermarked_df = events_df.withWatermark("event_time", "2 minutes")

# Filter: Ch·ªâ l·∫•y xe ƒë√£ PARKED (b·ªè ENTERING v√¨ ch∆∞a t√≠nh ph√≠)
events_billable = watermarked_df.filter(
    col("status_code").isin("PARKED", "MOVING", "EXITING")
)

# ƒê√°nh d·∫•u th·ªùi ƒëi·ªÉm PARKED (ƒë·ªÉ l·∫•y entry_time)
events_marked = events_billable.withColumn(
    "parked_time",
    when(col("status_code") == "PARKED", col("timestamp_unix"))
).withColumn(
    "parked_timestamp_str",
    when(col("status_code") == "PARKED", col("timestamp"))
)

# ===========================
# AGGREGATION: GroupBy (location, license_plate) - T·ªïng h·ª£p theo V·ªä TR√ç v√† XE
# ===========================
# FIX: GroupBy theo location ƒë·ªÉ track xe t·∫°i t·ª´ng v·ªã tr√≠
# Watermark s·∫Ω t·ª± ƒë·ªông x√≥a state sau khi EXITING (sau 2 ph√∫t)
# Khi xe v√†o l·∫°i, s·∫Ω t·∫°o state m·ªõi v·ªõi entry_time m·ªõi

vehicle_aggregated = events_marked.groupBy(
    col("location"),
    col("license_plate")
).agg(
    # Status m·ªõi nh·∫•t
    last("status_code").alias("current_status"),
    
    # ‚úÖ ENTRY TIME: L·∫ßn PARKED ƒê·∫¶U TI√äN t·∫°i v·ªã tr√≠ n√†y
    # min() ch·ªâ t√≠nh tr√™n c√°c event PARKED (v√¨ MOVING c√≥ parked_time = NULL)
    # Watermark s·∫Ω t·ª± ƒë·ªông x√≥a state sau khi EXITING (sau 2 ph√∫t)
    min("parked_time").alias("entry_time"),
    
    # Entry timestamp string
    min("parked_timestamp_str").alias("entry_timestamp"),
    
    # Last update ƒë·ªÉ t√≠nh th·ªùi gian ƒë·ªó
    max("timestamp_unix").alias("last_update"),
    
    # Exit timestamp (khi EXITING)
    max(when(col("status_code") == "EXITING", col("timestamp"))).alias("exit_timestamp")
)

# ===========================
# T√çNH PH√ç
# ===========================
parking_with_fees = vehicle_aggregated.withColumn(
    "parked_seconds",
    when(col("entry_time").isNotNull(), 
         col("last_update") - col("entry_time")
    ).otherwise(0)
).withColumn(
    "parked_minutes",
    (col("parked_seconds") / 60).cast("int")
).withColumn(
    "fee_blocks",
    when(col("parked_minutes") > 0,
         ((col("parked_minutes") + 9) / 10).cast("int")
    ).otherwise(0)
).withColumn(
    "total_fee",
    col("fee_blocks") * lit(PRICE_PER_10MIN)
)

# ===========================
# OUTPUT 1: REAL-TIME MONITORING (xe ƒëang ƒë·ªó)
# ===========================
# Ch·ªâ hi·ªÉn th·ªã xe ƒêANG ƒê·ªñ (PARKED/MOVING), filter b·ªè EXITING
# Watermark s·∫Ω t·ª± ƒë·ªông x√≥a state sau khi EXITING (sau 2 ph√∫t)
active_vehicles = parking_with_fees.select(
    col("location"),
    col("license_plate"),
    col("current_status"),
    col("entry_timestamp"),
    col("parked_minutes"),
    col("total_fee"),
    col("last_update")
).filter(
    # Xe ƒë√£ PARKED √≠t nh·∫•t 1 l·∫ßn V√Ä ch∆∞a EXITING
    (col("entry_time").isNotNull()) &
    (col("current_status") != "EXITING")
).orderBy("location")

# T·∫°o dataset cho Parquet (bao g·ªìm c·∫£ EXITING ƒë·ªÉ API c√≥ th·ªÉ x·ª≠ l√Ω)
# FIX: Parquet output bao g·ªìm c·∫£ EXITING ƒë·ªÉ API c√≥ th·ªÉ x·ª≠ l√Ω checkout events
all_vehicles_for_parquet = parking_with_fees.select(
    col("location"),
    col("license_plate"),
    col("current_status"),
    col("entry_timestamp"),
    col("parked_minutes"),
    col("total_fee"),
    col("last_update")
).filter(
    # Ch·ªâ filter b·ªè nh·ªØng xe ch∆∞a c√≥ entry_time
    col("entry_time").isNotNull()
).orderBy("location")

# Stream 1: Memory table cho monitoring (n·∫øu c·∫ßn query t·ª´ c√πng process)
memory_query = active_vehicles.writeStream \
    .outputMode("complete") \
    .format("memory") \
    .queryName("parking_realtime") \
    .trigger(processingTime="5 seconds") \
    .start()

# Stream 1b: Ghi ra Parquet file ƒë·ªÉ share v·ªõi API server
# FIX: Memory table kh√¥ng share ƒë∆∞·ª£c gi·ªØa c√°c SparkSession
# D√πng Parquet file ƒë·ªÉ share data gi·ªØa processes
# Parquet kh√¥ng h·ªó tr·ª£ "complete" mode, d√πng foreachBatch ƒë·ªÉ ghi l·∫°i to√†n b·ªô data
parquet_output_path = "/opt/shared/parking_realtime_parquet"

def write_to_parquet(batch_df, batch_id):
    """Ghi l·∫°i to√†n b·ªô data v√†o Parquet file (complete mode behavior)"""
    if not batch_df.isEmpty():
        # FIX: Ghi v√†o file t·∫°m r·ªìi rename ƒë·ªÉ tr√°nh race condition
        import shutil
        import time as time_module
        
        # Reload price config m·ªói batch ƒë·ªÉ c·∫≠p nh·∫≠t gi√° m·ªõi
        global PRICE_PER_10MIN
        PRICE_PER_10MIN = load_price_config()
        
        temp_path = f"{parquet_output_path}_temp_{int(time_module.time())}"
        final_path = parquet_output_path
        
        try:
            # X√≥a file t·∫°m c≈© n·∫øu c√≥ (cleanup)
            for old_temp in os.listdir(os.path.dirname(final_path) or '.'):
                if old_temp.startswith(os.path.basename(final_path) + "_temp_"):
                    old_path = os.path.join(os.path.dirname(final_path) or '.', old_temp)
                    if os.path.isdir(old_path):
                        try:
                            shutil.rmtree(old_path)
                        except:
                            pass
            
            # Ghi v√†o file t·∫°m v·ªõi timestamp unique
            batch_df.coalesce(1).write.mode("overwrite").parquet(temp_path)
            
            # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o file ƒë√£ ƒë∆∞·ª£c ghi xong
            time_module.sleep(0.1)
            
            # X√≥a file c≈© v√† rename file t·∫°m (atomic operation)
            if os.path.exists(final_path):
                shutil.rmtree(final_path)
            
            # Rename atomic
            os.rename(temp_path, final_path)
            
            logger.debug(f"Updated Parquet file at {final_path} (batch {batch_id}, {batch_df.count()} rows, Price: {PRICE_PER_10MIN:,} VND/10min)")
        except Exception as e:
            logger.error(f"Error writing Parquet file: {e}")
            # Cleanup temp file n·∫øu c√≥ l·ªói
            if os.path.exists(temp_path):
                try:
                    shutil.rmtree(temp_path)
                except:
                    pass

# FIX: D√πng all_vehicles_for_parquet ƒë·ªÉ bao g·ªìm c·∫£ EXITING
parquet_query = all_vehicles_for_parquet.writeStream \
    .foreachBatch(write_to_parquet) \
    .outputMode("complete") \
    .trigger(processingTime="5 seconds") \
    .start()
logger.info(f"‚úì Parquet output started: {parquet_output_path} (includes EXITING)")

# Stream 2: Console cho monitoring
console_query = active_vehicles.writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("truncate", "false") \
    .option("numRows", 30) \
    .trigger(processingTime="10 seconds") \
    .start()

# ===========================
# OUTPUT 2: CHECKOUT EVENTS (xe v·ª´a EXITING)
# ===========================
# L·∫•y xe EXITING ƒë·ªÉ l∆∞u v√†o DB/Kafka v√† reset state
checkout_records = parking_with_fees.select(
    col("license_plate"),
    col("location").alias("exit_location"),  # FIX: D√πng location thay v√¨ current_location
    col("entry_timestamp"),
    col("exit_timestamp"),
    col("parked_minutes"),
    col("total_fee"),
    current_timestamp().alias("processed_time")
).filter(
    (col("current_status") == "EXITING") &  # Ch·ªâ l·∫•y xe EXITING
    (col("entry_time").isNotNull())  # C√≥ entry time h·ª£p l·ªá
)

# Stream 3: foreachBatch ƒë·ªÉ x·ª≠ l√Ω checkout events
def process_checkout_batch(batch_df, batch_id):
    """X·ª≠ l√Ω checkout events: in ra console v√† c√≥ th·ªÉ g·ª≠i v√†o Kafka/DB"""
    if not batch_df.isEmpty():
        print(f"\n{'='*80}")
        print(f"üõí CHECKOUT EVENTS - Batch {batch_id}")
        print(f"{'='*80}")
        batch_df.show(truncate=False)
        print(f"{'='*80}\n")
        
        # TODO: C√≥ th·ªÉ th√™m logic ƒë·ªÉ:
        # 1. G·ª≠i v√†o Kafka topic kh√°c
        # 2. Insert v√†o database
        # 3. Reset state (n·∫øu c·∫ßn)

checkout_query = checkout_records.writeStream \
    .foreachBatch(process_checkout_batch) \
    .outputMode("update") \
    .trigger(processingTime="5 seconds") \
    .start()

# ===========================
# INFO
# ===========================
print("\n" + "=" * 80)
print("üé• CAMERA-BASED PARKING SYSTEM")
print("=" * 80)
print(f"Kafka: {KAFKA_BOOTSTRAP} / {KAFKA_TOPIC}")
print(f"Price: {PRICE_PER_10MIN:,} VND per 10 minutes")
print("=" * 80)
print("\nüìπ Logic:")
print("  ‚Ä¢ ENTERING ‚Üí No fee (waiting for parking)")
print("  ‚Ä¢ PARKED ‚Üí START billing from here")
print("  ‚Ä¢ MOVING ‚Üí Keep billing (still inside)")
print("  ‚Ä¢ PARKED again ‚Üí Continue billing from first PARKED")
print("  ‚Ä¢ EXITING ‚Üí Final checkout (sent to checkout stream)")
print("\nüîë GroupBy: license_plate (track per vehicle)")
print("   Each vehicle has: current_location, entry_time, total_fee")
print("\nüìä Outputs:")
print("  1. parking_realtime (memory) ‚Üí Active vehicles (PARKED/MOVING)")
print("  2. checkout_events (foreachBatch) ‚Üí EXITING events (for DB/Kafka)")
print("=" * 80 + "\n")

print("Example Flow:")
print("  00:10 - 29A-12345 ENTERING ‚Üí No fee")
print("  00:15 - 29A-12345 PARKED at A1 ‚Üí Start fee (entry_time = 00:15)")
print("           ‚Üì Appears in parking_realtime table")
print("  00:25 - 29A-12345 MOVING ‚Üí Fee = 10min (still billing)")
print("           ‚Üì Still in parking_realtime")
print("  00:30 - 29A-12345 PARKED at B5 ‚Üí Fee = 15min (continue from 00:15)")
print("           ‚Üì Still in parking_realtime")
print("  00:45 - 29A-12345 EXITING ‚Üí Final fee = 30min")
print("           ‚Üì Removed from parking_realtime")
print("           ‚Üì Sent to checkout_events stream (for DB/Kafka)")
print("           ‚Üì State ready for next visit (if needed)")
print("=" * 80 + "\n")

# ===========================
# RUN
# ===========================
try:
    spark.streams.awaitAnyTermination()
except KeyboardInterrupt:
    print("\nüõë Stopping...")
    for query in spark.streams.active:
        query.stop()
    spark.stop()
    print("‚úÖ Stopped.")