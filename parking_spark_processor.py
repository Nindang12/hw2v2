"""
FILE: parking_spark_processor.py
M√î T·∫¢: Camera-based Parking Fee Calculator

LOGIC CAMERA TH·ª∞C T·∫æ:
- Camera ·ªü m·ªói v·ªã tr√≠ (A1, A2, B1, ...) detect xe
- ENTERING: Xe v√†o c·ªïng ‚Üí CH∆ØA t√≠nh ph√≠
- PARKED t·∫°i A1: Camera A1 detect xe ‚Üí B·∫ÆT ƒê·∫¶U t√≠nh ph√≠
- MOVING: Camera kh√¥ng th·∫•y xe n·ªØa ‚Üí V·∫™N t√≠nh ph√≠ (xe di chuy·ªÉn trong b√£i)
- PARKED t·∫°i B5: Camera B5 detect xe ‚Üí TI·∫æP T·ª§C t√≠nh t·ª´ l·∫ßn PARKED ƒë·∫ßu
- EXITING: Xe ra ‚Üí CH·ªêT ph√≠

GroupBy license_plate: Track xe theo bi·ªÉn s·ªë
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import logging
import os
import json
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===========================
# CONFIG
# ===========================
CONFIG_FILE = "/opt/shared/parking_config.json"
LEGACY_PRICE_CONFIG_FILE = "/opt/shared/parking_price_config.json"  # Backward compatibility

# Default values
DEFAULT_CONFIG = {
    "price": {"price_per_10min": 5000, "currency": "VND"},
    "kafka": {"bootstrap_servers": "192.168.80.98:9092", "topic": "parking-events"},
    "spark": {"master": "spark://192.168.80.98:7077", "checkpoint_dir": "/tmp/checkpoint_parking", "shuffle_partitions": 2},
    "streaming": {"processing_interval_seconds": 5, "console_interval_seconds": 10}
}

def load_config():
    """Load config t·ª´ JSON file"""
    config = DEFAULT_CONFIG.copy()
    
    # Try to load from new config file
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r') as f:
                file_config = json.load(f)
                # Merge v·ªõi default config
                for key in config:
                    if key in file_config:
                        config[key].update(file_config[key])
                logger.info(f"Loaded config from {CONFIG_FILE}")
        except Exception as e:
            logger.warning(f"Error loading config from {CONFIG_FILE}: {e}, using defaults")
    
    # Backward compatibility: check legacy price config file
    if os.path.exists(LEGACY_PRICE_CONFIG_FILE):
        try:
            with open(LEGACY_PRICE_CONFIG_FILE, 'r') as f:
                legacy_config = json.load(f)
                if "price_per_10min" in legacy_config:
                    config["price"]["price_per_10min"] = legacy_config["price_per_10min"]
                    logger.info(f"Loaded price from legacy config: {config['price']['price_per_10min']:,} VND")
        except Exception as e:
            logger.warning(f"Error loading legacy price config: {e}")
    
    return config

def save_config(config):
    """L∆∞u config v√†o JSON file"""
    try:
        os.makedirs(os.path.dirname(CONFIG_FILE), exist_ok=True)
        with open(CONFIG_FILE, 'w') as f:
            json.dump(config, f, indent=2)
        logger.info(f"Saved config to {CONFIG_FILE}")
        return True
    except Exception as e:
        logger.error(f"Error saving config: {e}")
        return False

def update_price_in_config(new_price):
    """C·∫≠p nh·∫≠t gi√° trong config file"""
    config = load_config()
    config["price"]["price_per_10min"] = int(new_price)
    config["price"]["updated_at"] = datetime.now().isoformat()
    return save_config(config)

# Load config on startup
app_config = load_config()
KAFKA_BOOTSTRAP = app_config["kafka"]["bootstrap_servers"]
KAFKA_TOPIC = app_config["kafka"]["topic"]
PRICE_PER_10MIN = app_config["price"]["price_per_10min"]
CHECKPOINT_DIR = app_config["spark"]["checkpoint_dir"]
SPARK_MASTER = app_config["spark"]["master"]
SHUFFLE_PARTITIONS = app_config["spark"]["shuffle_partitions"]
PROCESSING_INTERVAL = app_config["streaming"]["processing_interval_seconds"]
CONSOLE_INTERVAL = app_config["streaming"]["console_interval_seconds"]

logger.info(f"Config loaded: Price={PRICE_PER_10MIN:,} VND/10min, Kafka={KAFKA_BOOTSTRAP}, Topic={KAFKA_TOPIC}")

# ===========================
# SPARK
# ===========================
spark = SparkSession.builder \
    .appName("ParkingCameraSystem") \
    .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_DIR) \
    .config("spark.sql.shuffle.partitions", str(SHUFFLE_PARTITIONS)) \
    .config("spark.sql.adaptive.enabled", "false") \
    .config("spark.hadoop.fs.defaultFS", "file:///") \
    .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem") \
    .config("spark.hadoop.fs.AbstractFileSystem.file.impl", "org.apache.hadoop.fs.local.LocalFs") \
    .master(SPARK_MASTER) \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ===========================
# SCHEMA & READ KAFKA
# ===========================
kafka_schema = StructType([
    StructField("timestamp", StringType(), True),
    StructField("timestamp_unix", LongType(), True),
    StructField("license_plate", StringType(), True),
    StructField("location", StringType(), True),
    StructField("status_code", StringType(), True)
])

kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load()

parsed_df = kafka_df.select(
    from_json(col("value").cast("string"), kafka_schema).alias("data")
).select("data.*")

events_df = parsed_df.withColumn(
    "event_time",
    to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss")
)

# ===========================
# LOGIC: B·ªè ENTERING, ch·ªâ x·ª≠ l√Ω t·ª´ PARKED tr·ªü ƒëi
# ===========================
watermarked_df = events_df.withWatermark("event_time", "2 minutes")

# Filter: Ch·ªâ l·∫•y xe ƒë√£ PARKED (b·ªè ENTERING v√¨ ch∆∞a t√≠nh ph√≠)
events_billable = watermarked_df.filter(
    col("status_code").isin("PARKED", "MOVING", "EXITING")
)

# ƒê√°nh d·∫•u th·ªùi ƒëi·ªÉm PARKED (ƒë·ªÉ l·∫•y entry_time)
events_marked = events_billable.withColumn(
    "parked_time",
    when(col("status_code") == "PARKED", col("timestamp_unix"))
).withColumn(
    "parked_timestamp_str",
    when(col("status_code") == "PARKED", col("timestamp"))
)

# ===========================
# AGGREGATION: GroupBy (location, license_plate) - T·ªïng h·ª£p theo V·ªä TR√ç v√† XE
# ===========================
# FIX: GroupBy theo location ƒë·ªÉ track xe t·∫°i t·ª´ng v·ªã tr√≠
# Watermark s·∫Ω t·ª± ƒë·ªông x√≥a state sau khi EXITING (sau 2 ph√∫t)
# Khi xe v√†o l·∫°i, s·∫Ω t·∫°o state m·ªõi v·ªõi entry_time m·ªõi

vehicle_aggregated = events_marked.groupBy(
    col("location"),
    col("license_plate")
).agg(
    # Status m·ªõi nh·∫•t
    last("status_code").alias("current_status"),
    
    # ‚úÖ ENTRY TIME: L·∫ßn PARKED ƒê·∫¶U TI√äN t·∫°i v·ªã tr√≠ n√†y
    # min() ch·ªâ t√≠nh tr√™n c√°c event PARKED (v√¨ MOVING c√≥ parked_time = NULL)
    # Watermark s·∫Ω t·ª± ƒë·ªông x√≥a state sau khi EXITING (sau 2 ph√∫t)
    min("parked_time").alias("entry_time"),
    
    # Entry timestamp string
    min("parked_timestamp_str").alias("entry_timestamp"),
    
    # Last update ƒë·ªÉ t√≠nh th·ªùi gian ƒë·ªó
    max("timestamp_unix").alias("last_update"),
    
    # Exit timestamp (khi EXITING)
    max(when(col("status_code") == "EXITING", col("timestamp"))).alias("exit_timestamp")
)

# ===========================
# T√çNH PH√ç
# ===========================
parking_with_fees = vehicle_aggregated.withColumn(
    "parked_seconds",
    when(col("entry_time").isNotNull(), 
         col("last_update") - col("entry_time")
    ).otherwise(0)
).withColumn(
    "parked_minutes",
    (col("parked_seconds") / 60).cast("int")
).withColumn(
    "fee_blocks",
    when(col("parked_minutes") > 0,
         ((col("parked_minutes") + 9) / 10).cast("int")
    ).otherwise(0)
).withColumn(
    "total_fee",
    col("fee_blocks") * lit(PRICE_PER_10MIN)
)

# ===========================
# OUTPUT 1: REAL-TIME MONITORING (xe ƒëang ƒë·ªó)
# ===========================
# Ch·ªâ hi·ªÉn th·ªã xe ƒêANG ƒê·ªñ (PARKED/MOVING), filter b·ªè EXITING
# Watermark s·∫Ω t·ª± ƒë·ªông x√≥a state sau khi EXITING (sau 2 ph√∫t)
active_vehicles = parking_with_fees.select(
    col("location"),
    col("license_plate"),
    col("current_status"),
    col("entry_timestamp"),
    col("parked_minutes"),
    col("total_fee"),
    col("last_update")
).filter(
    # Xe ƒë√£ PARKED √≠t nh·∫•t 1 l·∫ßn V√Ä ch∆∞a EXITING
    (col("entry_time").isNotNull()) &
    (col("current_status") != "EXITING")
).orderBy("location")

# T·∫°o dataset cho Parquet (bao g·ªìm c·∫£ EXITING ƒë·ªÉ API c√≥ th·ªÉ x·ª≠ l√Ω)
# FIX: Parquet output bao g·ªìm c·∫£ EXITING ƒë·ªÉ API c√≥ th·ªÉ x·ª≠ l√Ω checkout events
all_vehicles_for_parquet = parking_with_fees.select(
    col("location"),
    col("license_plate"),
    col("current_status"),
    col("entry_timestamp"),
    col("parked_minutes"),
    col("total_fee"),
    col("last_update")
).filter(
    # Ch·ªâ filter b·ªè nh·ªØng xe ch∆∞a c√≥ entry_time
    col("entry_time").isNotNull()
).orderBy("location")

# Stream 1: Memory table cho monitoring (n·∫øu c·∫ßn query t·ª´ c√πng process)
memory_query = active_vehicles.writeStream \
    .outputMode("complete") \
    .format("memory") \
    .queryName("parking_realtime") \
    .trigger(processingTime=f"{PROCESSING_INTERVAL} seconds") \
    .start()

# Stream 1b: Ghi ra Parquet file ƒë·ªÉ share v·ªõi API server
# FIX: Memory table kh√¥ng share ƒë∆∞·ª£c gi·ªØa c√°c SparkSession
# D√πng Parquet file ƒë·ªÉ share data gi·ªØa processes
# Parquet kh√¥ng h·ªó tr·ª£ "complete" mode, d√πng foreachBatch ƒë·ªÉ ghi l·∫°i to√†n b·ªô data
parquet_output_path = "/opt/shared/parking_realtime_parquet"

def write_to_parquet(batch_df, batch_id):
    """Ghi l·∫°i to√†n b·ªô data v√†o Parquet file (complete mode behavior)"""
    if not batch_df.isEmpty():
        # FIX: Ghi v√†o file t·∫°m r·ªìi rename ƒë·ªÉ tr√°nh race condition
        import shutil
        import time as time_module
        
        # Reload price config m·ªói batch ƒë·ªÉ c·∫≠p nh·∫≠t gi√° m·ªõi
        global PRICE_PER_10MIN
        app_config = load_config()
        PRICE_PER_10MIN = app_config["price"]["price_per_10min"]
        
        temp_path = f"{parquet_output_path}_temp_{int(time_module.time())}"
        final_path = parquet_output_path
        
        try:
            # X√≥a file t·∫°m c≈© n·∫øu c√≥ (cleanup)
            for old_temp in os.listdir(os.path.dirname(final_path) or '.'):
                if old_temp.startswith(os.path.basename(final_path) + "_temp_"):
                    old_path = os.path.join(os.path.dirname(final_path) or '.', old_temp)
                    if os.path.isdir(old_path):
                        try:
                            shutil.rmtree(old_path)
                        except:
                            pass
            
            # Ghi v√†o file t·∫°m v·ªõi timestamp unique
            batch_df.coalesce(1).write.mode("overwrite").parquet(temp_path)
            
            # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o file ƒë√£ ƒë∆∞·ª£c ghi xong ho√†n to√†n
            time_module.sleep(0.5)
            
            # FIX: Ki·ªÉm tra file t·∫°m ƒë√£ t·ªìn t·∫°i v√† c√≥ d·ªØ li·ªáu tr∆∞·ªõc khi x√≥a file c≈©
            if not os.path.exists(temp_path):
                raise Exception(f"Temp file {temp_path} was not created")
            
            # Ki·ªÉm tra file t·∫°m c√≥ √≠t nh·∫•t 1 file parquet b√™n trong
            temp_files = [f for f in os.listdir(temp_path) if f.endswith('.parquet')]
            if not temp_files:
                raise Exception(f"Temp file {temp_path} does not contain parquet files")
            
            # X√≥a file c≈© v√† rename file t·∫°m (atomic operation)
            # FIX: Ch·ªâ x√≥a file c≈© sau khi file t·∫°m ƒë√£ s·∫µn s√†ng
            if os.path.exists(final_path):
                try:
                    shutil.rmtree(final_path)
                    # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o x√≥a ho√†n t·∫•t
                    time_module.sleep(0.2)
                except Exception as e:
                    logger.warning(f"Error removing old file {final_path}: {e}")
            
            # Rename atomic
            os.rename(temp_path, final_path)
            
            # ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o rename ho√†n t·∫•t
            time_module.sleep(0.2)
            
            logger.debug(f"Updated Parquet file at {final_path} (batch {batch_id}, {batch_df.count()} rows, Price: {PRICE_PER_10MIN:,} VND/10min)")
        except Exception as e:
            logger.error(f"Error writing Parquet file: {e}")
            # Cleanup temp file n·∫øu c√≥ l·ªói
            if os.path.exists(temp_path):
                try:
                    shutil.rmtree(temp_path)
                except:
                    pass

# FIX: D√πng all_vehicles_for_parquet ƒë·ªÉ bao g·ªìm c·∫£ EXITING
parquet_query = all_vehicles_for_parquet.writeStream \
    .foreachBatch(write_to_parquet) \
    .outputMode("complete") \
    .trigger(processingTime=f"{PROCESSING_INTERVAL} seconds") \
    .start()
logger.info(f"‚úì Parquet output started: {parquet_output_path} (includes EXITING)")

# Stream 2: Console cho monitoring
console_query = active_vehicles.writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("truncate", "false") \
    .option("numRows", 30) \
    .trigger(processingTime=f"{CONSOLE_INTERVAL} seconds") \
    .start()

# ===========================
# OUTPUT 2: CHECKOUT EVENTS (xe v·ª´a EXITING)
# ===========================
# L·∫•y xe EXITING ƒë·ªÉ l∆∞u v√†o DB/Kafka v√† reset state
checkout_records = parking_with_fees.select(
    col("license_plate"),
    col("location").alias("exit_location"),  # FIX: D√πng location thay v√¨ current_location
    col("entry_timestamp"),
    col("exit_timestamp"),
    col("parked_minutes"),
    col("total_fee"),
    current_timestamp().alias("processed_time")
).filter(
    (col("current_status") == "EXITING") &  # Ch·ªâ l·∫•y xe EXITING
    (col("entry_time").isNotNull())  # C√≥ entry time h·ª£p l·ªá
)

# Stream 3: foreachBatch ƒë·ªÉ x·ª≠ l√Ω checkout events
def process_checkout_batch(batch_df, batch_id):
    """X·ª≠ l√Ω checkout events: in ra console v√† c√≥ th·ªÉ g·ª≠i v√†o Kafka/DB"""
    if not batch_df.isEmpty():
        print(f"\n{'='*80}")
        print(f"üõí CHECKOUT EVENTS - Batch {batch_id}")
        print(f"{'='*80}")
        batch_df.show(truncate=False)
        print(f"{'='*80}\n")
        
        # TODO: C√≥ th·ªÉ th√™m logic ƒë·ªÉ:
        # 1. G·ª≠i v√†o Kafka topic kh√°c
        # 2. Insert v√†o database
        # 3. Reset state (n·∫øu c·∫ßn)

checkout_query = checkout_records.writeStream \
    .foreachBatch(process_checkout_batch) \
    .outputMode("update") \
    .trigger(processingTime=f"{PROCESSING_INTERVAL} seconds") \
    .start()

# ===========================
# INFO
# ===========================
print("\n" + "=" * 80)
print("üé• CAMERA-BASED PARKING SYSTEM")
print("=" * 80)
print(f"Kafka: {KAFKA_BOOTSTRAP} / {KAFKA_TOPIC}")
print(f"Price: {PRICE_PER_10MIN:,} VND per 10 minutes")
print("=" * 80)
print("\nüìπ Logic:")
print("  ‚Ä¢ ENTERING ‚Üí No fee (waiting for parking)")
print("  ‚Ä¢ PARKED ‚Üí START billing from here")
print("  ‚Ä¢ MOVING ‚Üí Keep billing (still inside)")
print("  ‚Ä¢ PARKED again ‚Üí Continue billing from first PARKED")
print("  ‚Ä¢ EXITING ‚Üí Final checkout (sent to checkout stream)")
print("\nüîë GroupBy: license_plate (track per vehicle)")
print("   Each vehicle has: current_location, entry_time, total_fee")
print("\nüìä Outputs:")
print("  1. parking_realtime (memory) ‚Üí Active vehicles (PARKED/MOVING)")
print("  2. checkout_events (foreachBatch) ‚Üí EXITING events (for DB/Kafka)")
print("=" * 80 + "\n")

print("Example Flow:")
print("  00:10 - 29A-12345 ENTERING ‚Üí No fee")
print("  00:15 - 29A-12345 PARKED at A1 ‚Üí Start fee (entry_time = 00:15)")
print("           ‚Üì Appears in parking_realtime table")
print("  00:25 - 29A-12345 MOVING ‚Üí Fee = 10min (still billing)")
print("           ‚Üì Still in parking_realtime")
print("  00:30 - 29A-12345 PARKED at B5 ‚Üí Fee = 15min (continue from 00:15)")
print("           ‚Üì Still in parking_realtime")
print("  00:45 - 29A-12345 EXITING ‚Üí Final fee = 30min")
print("           ‚Üì Removed from parking_realtime")
print("           ‚Üì Sent to checkout_events stream (for DB/Kafka)")
print("           ‚Üì State ready for next visit (if needed)")
print("=" * 80 + "\n")

# ===========================
# RUN
# ===========================
try:
    spark.streams.awaitAnyTermination()
except KeyboardInterrupt:
    print("\nüõë Stopping...")
    for query in spark.streams.active:
        query.stop()
    spark.stop()
    print("‚úÖ Stopped.")